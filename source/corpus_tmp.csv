Authors	Author(s) ID	Title	Year	Source title	DOI	Link	Abstract	Author Keywords	Index Keywords	References
"Lecun Y., Bengio Y., Hinton G."	55666793600;7003958245;7006699573;	Deep learning	2015	Nature	10.1038/nature14539	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930630277&doi=10.1038%2fnature14539&partnerID=40&md5=e324cb9ec992f892ebc74f3e06078083	"Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. © 2015 Macmillan Publishers Limited. All rights reserved."		data processing; data set; machine learning; parameterization; automatic speech recognition; classifier; deep learning; human; image processing; language processing; learning; learning algorithm; learning theory; machine learning; nonhuman; pattern recognition; priority journal; recognition; Review; speech discrimination; algorithm; artificial intelligence; artificial neural network; computer; language; trends; Algorithms; Artificial Intelligence; Computers; Language; Neural Networks (Computer)	"Krizhevsky, A., Sutskever, I., Hinton, G., ImageNet classification with deep convolutional neural networks (2012) Proc. Advances in Neural Information Processing Systems, 25, pp. 1090-1098; Farabet, C., Couprie, C., Najman, L., Lecun, Y., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1915-1929; Tompson, J., Jain, A., Lecun, Y., Bregler, C., Joint training of a convolutional network and a graphical model for human pose estimation (2014) Proc. Advances in Neural Information Processing Systems, 27, pp. 1799-1807; Szegedy, C., (2014) Going Deeper with Convolutions. Preprint at, , http://arxiv.org/abs/1409.4842; Mikolov, T., Deoras, A., Povey, D., Burget, L., Cernocky, J., Strategies for training large scale neural network language models (2011) Proc. Automatic Speech Recognition and Understanding, pp. 196-201; Hinton, G., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Processing Magazine, 29, pp. 82-97; Sainath, T., Mohamed, A.-R., Kingsbury, B., Ramabhadran, B., Deep convolutional neural networks for LVCSR (2013) Proc. Acoustics, Speech and Signal Processing, pp. 8614-8618; Ma, J., Sheridan, R.P., Liaw, A., Dahl, G.E., Svetnik, V., Deep neural nets as a method for quantitative structure-activity relationships (2015) J. Chem. Inf. Model., 55, pp. 263-274; Ciodaro, T., Deva, D., De Seixas, J., Damazio, D., Online particle detection with neural networks based on topological calorimetry information (2012) J. Phys. Conf. Series, 368, p. 012030; (2014) Higgs Boson Machine Learning Challenge, , https://www.kaggle.com/c/higgs-boson, Kaggle; Helmstaedter, M., Connectomic reconstruction of the inner plexiform layer in the mouse retina (2013) Nature, 500, pp. 168-174; Leung, M.K., Xiong, H.Y., Lee, L.J., Frey, B.J., Deep learning of the tissue-regulated splicing code (2014) Bioinformatics, 30, pp. i121-i129; Xiong, H.Y., The human splicing code reveals new insights into the genetic determinants of disease (2015) Science, 347, p. 6218; Collobert, R., Natural language processing (almost) from scratch (2011) J. Mach. Learn. Res., 12, pp. 2493-2537; Bordes, A., Chopra, S., Weston, J., Question answering with subgraph embeddings (2014) Proc. Empirical Methods in Natural Language Processing, , http://arxiv.org/abs/1406.3676v3; Jean, S., Cho, K., Memisevic, R., Bengio, Y., On using very large target vocabulary for neural machine translation (2015) Proc. ACL-IJCNLP Http://arxiv. Org/ abs/1412. 2007; Sutskever Vinyals, I.O., Le., Q.V., Sequence to sequence learning with neural networks (2014) Proc. Advances in Neural Information Processing Systems, 27, pp. 3104-3112; Bottou, L., Bousquet, O., The tradeoffs of large scale learning (2007) Proc. Advances in Neural Information Processing Systems, 20, pp. 161-168; Duda, R.O., Hart, P.E., (1973) Pattern Classification and Scene Analysis, , Wiley; Schölkopf, B., Smola, A., (2002) Learning with Kernels, , MIT Press; Bengio, Y., Delalleau, O., Le Roux, N., The curse of highly variable functions for local kernel machines (2005) Proc. Advances in Neural Information Processing Systems, 18, pp. 107-114; Selfridge, O.G., Pandemonium: A paradigm for learning in mechanisation of thought processes (1958) Proc. Symposium on Mechanisation of Thought Processes, pp. 513-526; Rosenblatt, F., (1957) The Perceptron-A Perceiving and Recognizing Automaton, , Tech. Rep. 85-460-1 (Cornell Aeronautical Laboratory; Werbos, P., (1974) Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, , PhD thesis, Harvard Univ; Parker, D.B., (1985) Learning Logic Report TR-47, , MIT Press; Lecun, Y., (1985) Une Procédure d'Apprentissage Pour Réseau À Seuil Assymétrique in Cognitiva 85: A la Frontière de l'Intelligence Artificielle des Sciences de la Connaissance et des Neurosciences, pp. 599-604. , in French; Rumelhart, D.E., Hinton, G.E., Williams, R.J., Learning representations by back-propagating errors (1986) Nature, 323, pp. 533-536; Glorot, X., Bordes, A., Bengio, Y., Deep sparse rectifier neural networks (2011) Proc. 14th International Conference on Artificial Intelligence and Statistics, pp. 315-323; Dauphin, Y., Identifying and attacking the saddle point problem in high-dimensional non-convex optimization (2014) Proc. Advances in Neural Information Processing Systems, 27, pp. 2933-2941; Choromanska, A., Henaff, M., Mathieu, M., Arous, G.B., Lecun, Y., The loss surface of multilayer networks (2014) Proc. Conference on AI and Statistics, , http://arxiv.org/abs/1412.0233; Hinton, G.E., What kind of graphical model is the brain (2005) Proc. 19th International Joint Conference on Artificial Intelligence, pp. 1765-1775; Hinton, G.E., Osindero, S., Teh, Y.-W., A fast learning algorithm for deep belief nets (2006) Neural Comp., 18, pp. 1527-1554; Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., Greedy layer-wise training of deep networks (2006) Proc. Advances in Neural Information Processing Systems, 19, pp. 153-160; Ranzato, M., Poultney, C., Chopra, S., Lecun, Y., Efficient learning of sparse representations with an energy-based model (2006) Proc. Advances in Neural Information Processing Systems, 19, pp. 1137-1144; Hinton, G.E., Salakhutdinov, R., Reducing the dimensionality of data with neural networks (2006) Science, 313, pp. 504-507; Sermanet, P., Kavukcuoglu, K., Chintala, S., Lecun, Y., Pedestrian detection with unsupervised multi-stage feature learning (2013) Proc. International Conference on Computer Vision and Pattern Recognition, , http://arxiv.org/abs/1212.0142; Raina, R., Madhavan, A., Ng, A.Y., Large-scale deep unsupervised learning using graphics processors (2009) Proc. 26th Annual International Conference on Machine Learning, pp. 873-880; Mohamed, A.-R., Dahl, G.E., Hinton, G., Acoustic modeling using deep belief networks (2012) IEEE Trans. Audio Speech Lang. Process., 20, pp. 14-22; Dahl, G.E., Yu, D., Deng, L., Acero, A., Context-dependent pre-trained deep neural networks for large vocabulary speech recognition (2012) IEEE Trans. Audio Speech Lang. Process., 20, pp. 33-42; Bengio, Y., Courville, A., Vincent, P., Representation learning: A review and new perspectives (2013) IEEE Trans. Pattern Anal. Machine Intell., 35, pp. 1798-1828; Lecun, Y., Handwritten digit recognition with a back-propagation network (1990) Proc. Advances in Neural Information Processing Systems, pp. 396-404; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86, pp. 2278-2324; Hubel, D.H., Wiesel, T.N., Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex (1962) J. Physiol., 160, pp. 106-154; Felleman, D.J., Essen, D.C.V., Distributed hierarchical processing in the primate cerebral cortex (1991) Cereb. Cortex, 1, pp. 1-47; Cadieu, C.F., Deep neural networks rival the representation of primate it cortex for core visual object recognition (2014) PLoS Comp. Biol., 10, p. e1003963; Fukushima, K., Miyake, S., Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position (1982) Pattern Recognition, 15, pp. 455-469; Waibel, A., Hanazawa, T., Hinton, G.E., Shikano, K., Lang, K., Phoneme recognition using time-delay neural networks (1989) IEEE Trans. Acoustics Speech Signal Process., 37, pp. 328-339; Bottou, L., Fogelman-Soulié, F., Blanchet, P., Lienard, J., Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition (1989) Proc. EuroSpeech, 89, pp. 537-540; Simard, D., Steinkraus, P.Y., Platt, J.C., Best practices for convolutional neural networks (2003) Proc. Document Analysis and Recognition, pp. 958-963; Vaillant, R., Monrocq, C., Lecun, Y., Original approach for the localisation of objects in images (1994) Proc. Vision, Image, and Signal Processing, 141, pp. 245-250; Nowlan, S., Platt, J., (1995) Neural Information Processing Systems, pp. 901-908; Lawrence, S., Giles, C.L., Tsoi, A.C., Back, A.D., Face recognition: A convolutional neural-network approach (1997) IEEE Trans. Neural Networks, 8, pp. 98-113; Ciresan, D., Meier Masci, U.J., Schmidhuber, J., Multi-column deep neural network for traffic sign classification (2012) Neural Networks, 32, pp. 333-338; Ning, F., Toward automatic phenotyping of developing embryos from videos (2005) IEEE Trans. Image Process., 14, pp. 1360-1371; Turaga, S.C., Convolutional networks can learn to generate affinity graphs for image segmentation (2010) Neural Comput., 22, pp. 511-538; Garcia, C., Delakis, M., Convolutional face finder: A neural architecture for fast and robust face detection (2004) IEEE Trans. Pattern Anal. Machine Intell., 26, pp. 1408-1423; Osadchy, M., Lecun, Y., Miller, M., Synergistic face detection and pose estimation with energy-based models (2007) J. Mach. Learn. Res., 8, pp. 1197-1215; Tompson, J., Goroshin, R.R., Jain, A., Lecun, Y.Y., Bregler, C.C., Efficient object localization using convolutional networks (2014) Proc. Conference on Computer Vision and Pattern Recognition, , http://arxiv.org/abs/1411.4280; Taigman, Y., Yang, M., Ranzato, M., Wolf, L., Deepface: Closing the gap to human-level performance in face verification (2014) Proc. Conference on Computer Vision and Pattern Recognition, pp. 1701-1708; Hadsell, R., Learning long-range vision for autonomous off-road driving (2009) J. Field Robot., 26, pp. 120-144; Farabet, C., Couprie, C., Najman, L., Lecun, Y., Scene parsing with multiscale feature learning, purity trees, and optimal covers (2012) Proc. International Conference on Machine Learning, , http://arxiv.org/abs/1202.2160; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) J. Machine Learning Res., 15, pp. 1929-1958; Sermanet, P., Overfeat: Integrated recognition, localization and detection using convolutional networks (2014) Proc. International Conference on Learning Representations, , http://arxiv.org/abs/1312.6229; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proc. Conference on Computer Vision and Pattern Recognition, pp. 580-587; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) Proc. International Conference on Learning Representations, , http://arxiv.org/abs/1409.1556; Boser, B., Sackinger, E., Bromley, J., Lecun, Y., Jackel, L., An analog neural network processor with programmable topology (1991) J. Solid State Circuits, 26, pp. 2017-2025; Farabet, C., Large-scale FPGA-based convolutional networks (2011) Scaling Up Machine Learning: Parallel and Distributed Approaches, pp. 399-419. , eds Bekkerman, R., Bilenko, M. & Langford, J. Cambridge Univ. Press; Bengio, Y., (2009) Learning Deep Architectures for AI (Now; Montufar, G., Morton, J., When does a mixture of products contain a product of mixtures (2014) J. Discrete Math., 29, pp. 321-347; Montufar, G.F., Pascanu, R., Cho, K., Bengio, Y., On the number of linear regions of deep neural networks (2014) Proc. Advances in Neural Information Processing Systems, 27, pp. 2924-2932; Bengio, Y., Ducharme, R., Vincent, P., A neural probabilistic language model (2001) Proc. Advances in Neural Information Processing Systems, 13, pp. 932-938; Cho, K., For statistical machine translation learning phrase representations using rnn encoder-decoder (2014) Proc Conference on Empirical Methods in Natural Language Processing, pp. 1724-1734; Schwenk, H., Continuous space language models (2007) Computer Speech Lang., 21, pp. 492-518; Socher, R., Lin, C.C.-Y., Manning, C., Ng, A.Y., Parsing natural scenes and natural language with recursive neural networks (2011) Proc. International Conference on Machine Learning, pp. 129-136; Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J., Distributed representations of words and phrases and their compositionality (2013) Proc. Advances in Neural Information Processing Systems, 26, pp. 3111-3119; Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2015) Proc. International Conference on Learning Representations, , http://arxiv.org/abs/1409.0473; Hochreiter, S., (1991) Untersuchungen zu Dynamischen Neuronalen Netzen, , German] Diploma thesis, T. U. Münich; Bengio, Y., Simard, P., Frasconi, P., Learning long-term dependencies with gradient descent is difficult (1994) IEEE Trans. Neural Networks, 5, pp. 157-166; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9, pp. 1735-1780; Elhihi, S., Bengio, Y., Hierarchical recurrent neural networks for long-term dependencies (1995) Proc. Advances in Neural Information Processing Systems, 8. , http://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies; Sutskever, I., (2012) Training Recurrent Neural Networks, , PhD thesis, Univ. Toronto; Pascanu, R., Mikolov, T., Bengio, Y., On the difficulty of training recurrent neural networks (2013) Proc. 30th International Conference on Machine Learning, pp. 1310-1318; Sutskever, I., Martens, J., Hinton, G.E., Generating text with recurrent neural networks (2011) Proc. 28th International Conference on Machine Learning, pp. 1017-1024; Lakoff, G., Johnson, M., Metaphors We Live by, 2008. , Univ Chicago Press; Rogers, T.T., McClelland, J.L., (2004) Semantic Cognition: A Parallel Distributed Processing Approach, , MIT Press; Xu, K., Show, attend and tell: Neural image caption generation with visual attention (2015) Proc. International Conference on Learning Representations, , http://arxiv.org/abs/1502.03044; Graves, A., Mohamed, A.-R., Hinton, G., Speech recognition with deep recurrent neural networks (2013) Proc. International Conference on Acoustics, Speech and Signal Processing, pp. 6645-6649; Graves, A., Wayne, G., Danihelka, I., (2014) Neural Turing Machines, , http://arxiv.org/abs/1410.5401; Weston Chopra, J.S., Bordes, A., (2014) Memory Networks, , http://arxiv.org/abs/1410.3916; Weston, J., Bordes, A., Chopra, S., Mikolov, T., (2015) Towards AI-complete Question Answering: A Set of Prerequisite Toy Tasks, , http://arxiv.org/abs/1502.05698; Hinton, G.E., Dayan, P., Frey, B.J., Neal, R.M., The wake-sleep algorithm for unsupervised neural networks (1995) Science, 268, pp. 1558-1161; Salakhutdinov, R., Hinton, G., Deep Boltzmann machines (2009) Proc. International Conference on Artificial Intelligence and Statistics, pp. 448-455; Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A., Extracting and composing robust features with denoising autoencoders (2008) Proc. 25th International Conference on Machine Learning, pp. 1096-1103; Kavukcuoglu, K., Learning convolutional feature hierarchies for visual recognition (2010) Proc. Advances in Neural Information Processing Systems, 23, pp. 1090-1098; Gregor, K., Lecun, Y., Learning fast approximations of sparse coding (2010) Proc. International Conference on Machine Learning, pp. 399-406; Ranzato, M., Mnih, V., Susskind, J.M., Hinton, G.E., Modeling natural images using gated MRFs (2013) IEEE Trans. Pattern Anal. Machine Intell., 35, pp. 2206-2222; Bengio, Y., Thibodeau-Laufer, E., Alain, G., Yosinski, J., Deep generative stochastic networks trainable by backprop (2014) Proc. 31st International Conference on Machine Learning, pp. 226-234; Kingma, D., Rezende, D., Mohamed, S., Welling, M., Semi-supervised learning with deep generative models (2014) Proc. Advances in Neural Information Processing Systems, 27, pp. 3581-3589; Ba, J., Mnih, V., Kavukcuoglu, K., Multiple object recognition with visual attention (2014) Proc. International Conference on Learning Representations, , http://arxiv.org/abs/1412.7755; Mnih, V., Human-level control through deep reinforcement learning (2015) Nature, 518, pp. 529-533; Bottou, L., From machine learning to machine reasoning (2014) Mach. Learn., 94, pp. 133-149; Vinyals, O., Toshev, A., Bengio, S., Erhan, D., Show and tell: A neural image caption generator (2014) Proc. International Conference on Machine Learning, , http://arxiv.org/abs/1502.03044; Van Der Maaten, L., Hinton, G.E., Visualizing data using t-SNE (2008) J. Mach. Learn. Research, 9, pp. 2579-2605"
Schmidhuber J.	7003514621;	Deep Learning in neural networks: An overview	2015	Neural Networks	10.1016/j.neunet.2014.09.003	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910651844&doi=10.1016%2fj.neunet.2014.09.003&partnerID=40&md5=1e380e40a7a616540c705ef8a63ce456	"In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks. © 2014."	Deep learning; Evolutionary computation; Reinforcement learning; Supervised learning; Unsupervised learning	Artificial intelligence; Backpropagation; Evolutionary algorithms; Neural networks; Pattern recognition; Recurrent neural networks; Supervised learning; Surveys; Unsupervised learning; Credit assignment; Deep learning; Large networks; Reinforcement learning; artificial intelligence; artificial neural network; automated pattern recognition; back propagation; computer language; deep learning; feedforward neural network; human; information processing; learning algorithm; machine learning; nnsupervised Learning; nonhuman; positive feedback; problem solving; process optimization; recurrent neural network; reinforcement learning; Review; short term memory; supervised learning; visual cortex; classification; standards; trends; Artificial Intelligence	"Aberdeen, D., (2003) Policy-gradient algorithms for partially observable Markov decision processes, , (Ph.D. thesis), Australian National University; Abounadi, J., Bertsekas, D., Borkar, V.S., Learning algorithms for Markov decision processes with average cost (2002) SIAM Journal on Control and Optimization, 40 (3), pp. 681-698; Akaike, H., Statistical predictor identification (1970) Annals of the Institute of Statistical Mathematics, 22, pp. 203-217; Akaike, H., Information theory and an extension of the maximum likelihood principle (1973) Second intl. symposium on information theory, pp. 267-281. , Akademinai Kiado; Akaike, H., A new look at the statistical model identification (1974) IEEE Transactions on Automatic Control, 19 (6), pp. 716-723; Allender, A., Application of time-bounded Kolmogorov complexity in complexity theory (1992) EATCS monographs on theoretical computer science, pp. 6-22. , Springer, O. Watanabe (Ed.) Kolmogorov complexity and computational complexity; Almeida, L.B., A learning rule for asynchronous perceptrons with feedback in a combinatorial environment (1987) IEEE 1st international conference on neural networks, 2, pp. 609-618; Almeida, L.B., Almeida, L.B., Langlois, T., Amaral, J.D., Redol, R.A., (1997) On-line step size adaptation. Technical report, INESC, 9 Rua Alves Redol, 1000; Amari, S., A theory of adaptive pattern classifiers (1967) IEEE Transactions on Electronic Computers, 16 (3), pp. 299-307; Amari, S.-I., Natural gradient works efficiently in learning (1998) Neural Computation, 10 (2), pp. 251-276; Amari, S., Cichocki, A., Yang, H., A new learning algorithm for blind signal separation (1996) Advances in neural information processing systems (NIPS), vol. 8, , The MIT Press, D.S. Touretzky, M.C. Mozer, M.E. Hasselmo (Eds.); Amari, S., Murata, N., Statistical theory of learning curves under entropic loss criterion (1993) Neural Computation, 5 (1), pp. 140-153; Amit, D.J., Brunel, N., Dynamics of a recurrent network of spiking neurons before and following learning (1997) Network: Computation in Neural Systems, 8 (4), pp. 373-404; An, G., The effects of adding noise during backpropagation training on a generalization performance (1996) Neural Computation, 8 (3), pp. 643-674; Andrade, M.A., Chacon, P., Merelo, J.J., Moran, F., Evaluation of secondary structure of proteins from UV circular dichroism spectra using an unsupervised learning neural network (1993) Protein Engineering, 6 (4), pp. 383-390; Andrews, R., Diederich, J., Tickle, A.B., Survey and critique of techniques for extracting rules from trained artificial neural networks (1995) Knowledge-Based Systems, 8 (6), pp. 373-389; Anguita, D., Gomes, B.A., Mixing floating- and fixed-point formats for neural network learning on neuroprocessors (1996) Microprocessing and Microprogramming, 41 (10), pp. 757-769; Anguita, D., Parodi, G., Zunino, R., An efficient implementation of BP on RISC-based workstations (1994) Neurocomputing, 6 (1), pp. 57-65; Arel, I., Rose, D.C., Karnowski, T.P., Deep machine learning-a new frontier in artificial intelligence research (2010) IEEE Computational Intelligence Magazine, 5 (4), pp. 13-18; Ash, T., Dynamic node creation in backpropagation neural networks (1989) Connection Science, 1 (4), pp. 365-375; Atick, J.J., Li, Z., Redlich, A.N., Understanding retinal color coding from first principles (1992) Neural Computation, 4, pp. 559-572; Atiya, A.F., Parlos, A.G., New results on recurrent network training: unifying the algorithms and accelerating convergence (2000) IEEE Transactions on Neural Networks, 11 (3), pp. 697-709; Ba, J., Frey, B., Adaptive dropout for training deep neural networks (2013) Advances in neural information processing systems (NIPS), pp. 3084-3092; Baird, H., (1990) Document image defect models, , Proceddings, IAPR workshop on syntactic and structural pattern recognition; Baird, L.C., Residual algorithms: Reinforcement learning with function approximation (1995) International conference on machine learning, pp. 30-37; Baird, L., Moore, A.W., Gradient descent for general reinforcement learning (1999) Advances in neural information processing systems, vol. 12 (NIPS), pp. 968-974. , MIT Press; Bakker, B., Reinforcement learning with long short-term memory (2002) Advances in neural information processing systems, vol. 14, pp. 1475-1482. , MIT Press, Cambridge, MA, T.G. Dietterich, S. Becker, Z. Ghahramani (Eds.); Bakker, B., Schmidhuber, J., Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization (2004) Proc. 8th conference on intelligent autonomous systems IAS-8, pp. 438-445. , IOS Press, Amsterdam, NL, F. Groen (Ed.); Bakker, B., Zhumatiy, V., Gruener, G., Schmidhuber, J., A robot that reinforcement-learns to identify and memorize important previous observations (2003), pp. 430-435. , Proceedings of the 2003 IEEE/RSJ international conference on intelligent robots and systems; Baldi, P., Gradient descent learning algorithms overview: A general dynamical systems perspective (1995) IEEE Transactions on Neural Networks, 6 (1), pp. 182-195; Baldi, P., Autoencoders, unsupervised learning, and deep architectures (2012) Journal of Machine Learning Research, 27, pp. 37-50. , (Proc. 2011 ICML Workshop on Unsupervised and Transfer Learning); Baldi, P., Brunak, S., Frasconi, P., Pollastri, G., Soda, G., Exploiting the past and the future in protein secondary structure prediction (1999) Bioinformatics, 15, pp. 937-946; Baldi, P., Chauvin, Y., Neural networks for fingerprint recognition (1993) Neural Computation, 5 (3), pp. 402-418; Baldi, P., Chauvin, Y., Hybrid modeling, HMM/NN architectures, and protein applications (1996) Neural Computation, 8 (7), pp. 1541-1565; Baldi, P., Hornik, K., Neural networks and principal component analysis: learning from examples without local minima (1989) Neural Networks, 2, pp. 53-58; Baldi, P., Hornik, K., Learning in linear networks: a survey (1995) IEEE Transactions on Neural Networks, 6 (4), pp. 837-858. , 1995; Baldi, P., Pollastri, G., The principled design of large-scale recursive neural network architectures-DAG-RNNs and the protein structure prediction problem (2003) Journal of Machine Learning Research, 4, pp. 575-602; Baldi, P., Sadowski, P., The dropout learning algorithm (2014) Artificial Intelligence, 210 C, pp. 78-122; Ballard, D.H., Modular learning in neural networks (1987) Proc. AAAI, pp. 279-284; Baluja, S., (1994) Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. Technical report CMU-CS-94-163, , Carnegie Mellon University; Balzer, R., A 15 year perspective on automatic programming (1985) IEEE Transactions on Software Engineering, 11 (11), pp. 1257-1268; Barlow, H.B., Unsupervised learning (1989) Neural Computation, 1 (3), pp. 295-311; Barlow, H.B., Kaushal, T.P., Mitchison, G.J., Finding minimum entropy codes (1989) Neural Computation, 1 (3), pp. 412-423; Barrow, H.G., Learning receptive fields (1987) Proceedings of the IEEE 1st annual conference on neural networks, vol. IV, pp. 115-121. , IEEE; Barto, A.G., Mahadevan, S., Recent advances in hierarchical reinforcement learning (2003) Discrete Event Dynamic Systems, 13 (4), pp. 341-379; Barto, A.G., Singh, S., Chentanez, N., Intrinsically motivated learning of hierarchical collections of skills (2004) Proceedings of international conference on developmental learning, pp. 112-119. , MIT Press, Cambridge, MA; Barto, A.G., Sutton, R.S., Anderson, C.W., Neuronlike adaptive elements that can solve difficult learning control problems (1983) IEEE Transactions on Systems, Man and Cybernetics, SMC-13, pp. 834-846; Battiti, R., Accelerated backpropagation learning: two optimization methods (1989) Complex Systems, 3 (4), pp. 331-342; Battiti, T., First- and second-order methods for learning: between steepest descent and Newton's method (1992) Neural Computation, 4 (2), pp. 141-166; Baum, E.B., Haussler, D., What size net gives valid generalization? (1989) Neural Computation, 1 (1), pp. 151-160; Baum, L.E., Petrie, T., Statistical inference for probabilistic functions of finite state Markov chains (1966) The Annals of Mathematical Statistics, pp. 1554-1563; Baxter, J., Bartlett, P.L., Infinite-horizon policy-gradient estimation (2001) Journal of Artificial Intelligence Research, 15 (1), pp. 319-350; Bayer, J., Osendorfer, C., (2014) Variational inference of latent state sequences using recurrent networks, , arxiv:1406.1655, ArXiv Preprint ; Bayer, J., Osendorfer, C., Chen, N., Urban, S., van der Smagt, P., (2013) On fast dropout and its applicability to recurrent networks, , arxiv:1311.0701. ArXiv Preprint; Bayer, J., Wierstra, D., Togelius, J., Schmidhuber, J., Evolving memory cell structures for sequence learning (2009) Proc. ICANN, (2), pp. 755-764; Bayes, T., An essay toward solving a problem in the doctrine of chances (1763) Philosophical Transactions of the Royal Society of London, 53, pp. 370-418. , Communicated by R. Price, in a letter to J. Canton; Becker, S., Unsupervised learning procedures for neural networks (1991) International Journal of Neural Systems, 2 (1-2), pp. 17-33; Becker, S., Le Cun, Y., Improving the convergence of back-propagation learning with second order methods (1989) Proc. 1988 connectionist models summer school, 1988, pp. 29-37. , Morgan Kaufmann, San Mateo, D. Touretzky, G. Hinton, T. Sejnowski (Eds.); Behnke, S., Hebbian learning and competition in the neural abstraction pyramid (1999) Proceedings of the international joint conference on neural networks, 2, pp. 1356-1361; Behnke, S., Learning iterative image reconstruction in the neural abstraction pyramid (2001) International Journal of Computational Intelligence and Applications, 1 (4), pp. 427-438; Behnke, S., Learning face localization using hierarchical recurrent networks. (2002) Proceedings of the 12th international conference on artificial neural networks, pp. 1319-1324; Behnke, S., Discovering hierarchical speech features using convolutional non-negative matrix factorization (2003) Proceedings of the international joint conference on neural networks, 4, pp. 2758-2763; Behnke, S., Hierarchical neural networks for image interpretation (2003) LNCS, Lecture notes in computer science, 2766. , Springer; Behnke, S., Face localization and tracking in the neural abstraction pyramid (2005) Neural Computing and Applications, 14 (2), pp. 97-103; Behnke, S., Rojas, R., Neural abstraction pyramid: a hierarchical image understanding architecture (1998) Proceedings of international joint conference on neural networks, 2, pp. 820-825; Bell, A.J., Sejnowski, T.J., An information-maximization approach to blind separation and blind deconvolution (1995) Neural Computation, 7 (6), pp. 1129-1159; Bellman, R., (1957) Dynamic programming, , Princeton University Press, Princeton, NJ, USA; Belouchrani, A., Abed-Meraim, K., Cardoso, J.-F., Moulines, E., A blind source separation technique using second-order statistics (1997) IEEE Transactions on Signal Processing, 45 (2), pp. 434-444; Bengio, Y., (1991) Artificial neural networks and their application to sequence recognition, , (Ph.D. thesis), McGill University, (Computer Science), Montreal, QC, Canada; Bengio, Y., Learning deep architectures for AI (2009) Foundations and trends in machine learning, 2 (1). , Now Publishers; Bengio, Y., Courville, A., Vincent, P., Representation learning: a review and new perspectives (2013) IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (8), pp. 1798-1828; Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., Greedy layer-wise training of deep networks (2007) Advances in neural information processing systems, vol. 19 (NIPS), pp. 153-160. , MIT Press, J.D. Cowan, G. Tesauro, J. Alspector (Eds.); Bengio, Y., Simard, P., Frasconi, P., Learning long-term dependencies with gradient descent is difficult (1994) IEEE Transactions on Neural Networks, 5 (2), pp. 157-166; Beringer, N., Graves, A., Schiel, F., Schmidhuber, J., Classifying unprompted speech by retraining LSTM nets (2005) LNCS, 3696, pp. 575-581. , Springer-Verlag, Berlin, Heidelberg, W. Duch, J. Kacprzyk, E. Oja, S. Zadrozny (Eds.) Artificial neural networks: biological inspirations-ICANN 2005; Bertsekas, D.P., (2001) Dynamic programming and optimal control, , Athena Scientific; Bertsekas, D.P., Tsitsiklis, J.N., (1996) Neuro-dynamic programming, , Athena Scientific, Belmont, MA; Bichot, N.P., Rossi, A.F., Desimone, R., Parallel and serial neural mechanisms for visual search in macaque area V4 (2005) Science, 308, pp. 529-534; Biegler-König, F., Bärmann, F., A learning algorithm for multilayered neural networks based on linear least squares problems (1993) Neural Networks, 6 (1), pp. 127-131; Bishop, C.M., Curvature-driven smoothing: A learning algorithm for feed-forward networks (1993) IEEE Transactions on Neural Networks, 4 (5), pp. 882-884; Bishop, C.M., (2006) Pattern recognition and machine learning, , Springer; Blair, A.D., Pollack, J.B., Analysis of dynamical recognizers (1997) Neural Computation, 9 (5), pp. 1127-1142; Blondel, V.D., Tsitsiklis, J.N., A survey of computational complexity results in systems and control (2000) Automatica, 36 (9), pp. 1249-1274; Bluche, T., Louradour, J., Knibbe, M., Moysset, B., Benzeghiba, F., Kermorvant, C., The A2iA Arabic handwritten text recognition system at the OpenHaRT2013 evaluation (2014) International workshop on document analysis systems.; Blum, A.L., Rivest, R.L., Training a 3-node neural network is NP-complete (1992) Neural Networks, 5 (1), pp. 117-127; Blumer, A., Ehrenfeucht, A., Haussler, D., Warmuth, M.K., Occam's razor (1987) Information Processing Letters, 24, pp. 377-380; Bobrowski, L., Learning processes in multilayer threshold nets (1978) Biological Cybernetics, 31, pp. 1-6; Bodén, M., Wiles, J., Context-free and context-sensitive dynamics in recurrent neural networks (2000) Connection Science, 12 (3-4), pp. 197-210; Bodenhausen, U., Waibel, A., The Tempo 2 algorithm: adjusting time-delays by supervised learning (1991) Advances in neural information processing systems, vol. 3, pp. 155-161. , Morgan Kaufmann, D.S. Lippman, J.E. Moody, D.S. Touretzky (Eds.); Bohte, S.M., Kok, J.N., La Poutre, H., Error-backpropagation in temporally encoded networks of spiking neurons (2002) Neurocomputing, 48 (1), pp. 17-37; Boltzmann, L., (1909) Wissenschaftliche Abhandlungen, , Barth, Leipzig, (collection of Boltzmann's articles in scientific journals), F. Hasenöhrl (Ed.); Bottou, L., (1991) Une approche théorique de l'apprentissage connexioniste; applications à la reconnaissance de la parole, , (Ph.D. thesis), Université de Paris XI; Bourlard, H., Morgan, N., (1994) Connnectionist speech recognition: a hybrid approach, , Kluwer Academic Publishers; Boutilier, C., Poole, D., Computing optimal policies for partially observable Markov decision processes using compact representations (1996) Proceedings of the AAAI.; Bradtke, S.J., Barto, A.G., Kaelbling, L.P., Linear least-squares algorithms for temporal difference learning (1996) Machine Learning, pp. 22-33; Brafman, R.I., Tennenholtz, M., R-MAX-a general polynomial time algorithm for near-optimal reinforcement learning (2002) Journal of Machine Learning Research, 3, pp. 213-231; Brea, J., Senn, W., Pfister, J.-P., Matching recall and storage in sequence learning with spiking neural networks (2013) The Journal of Neuroscience, 33 (23), pp. 9565-9575; Breiman, L., Bagging predictors (1996) Machine Learning, 24, pp. 123-140; Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J.M., Simulation of networks of spiking neurons: a review of tools and strategies (2007) Journal of Computational Neuroscience, 23 (3), pp. 349-398; Breuel, T.M., Ul-Hasan, A., Al-Azawi, M.A., Shafait, F., High-performance OCR for printed English and Fraktur using LSTM networks (2013) 12th International conference on document analysis and recognition, pp. 683-687. , IEEE; Bromley, J., Bentz, J.W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., Signature verification using a Siamese time delay neural network (1993) International Journal of Pattern Recognition and Artificial Intelligence, 7 (4), pp. 669-688; Broyden, C.G., A class of methods for solving nonlinear simultaneous equations (1965) Mathematics of Computation, 19 (92), pp. 577-593; Brueckner, R., Schulter, B., Social signal classification using deep BLSTM recurrent neural networks (2014) Proceedings 39th IEEE international conference on acoustics, speech, and signal processing, pp. 4856-4860; Brunel, N., Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons (2000) Journal of Computational Neuroscience, 8 (3), pp. 183-208; Bryson, A.E., A gradient method for optimizing multi-stage allocation processes (1961) Proc. Harvard Univ. symposium on digital computers and their applications.; Bryson, Jr.A.E., Denham, W.F., (1961) A steepest-ascent method for solving optimum programming problems. Technical report BR-1303, , Raytheon Company, Missle and Space Division; Bryson, A., Ho, Y., (1969) Applied optimal control: optimization, estimation, and control, , Blaisdell Pub. Co; Buhler, J., Efficient large-scale sequence comparison by locality-sensitive hashing (2001) Bioinformatics, 17 (5), pp. 419-428; Buntine, W.L., Weigend, A.S., Bayesian back-propagation (1991) Complex Systems, 5, pp. 603-643; Burgess, N., A constructive algorithm that converges for real-valued input patterns (1994) International Journal of Neural Systems, 5 (1), pp. 59-66; Cardoso, J.-F., On the performance of orthogonal source separation algorithms (1994) Proc. EUSIPCO, pp. 776-779; Carreira-Perpinan, M.A., (2001) Continuous latent variable models for dimensionality reduction and sequential data reconstruction, , (Ph.D. thesis), University of Sheffield, UK; Carter, M.J., Rudolph, F.J., Nucci, A.J., Operational fault tolerance of CMAC networks (1990) Advances in neural information processing systems (NIPS), vol. 2, pp. 340-347. , Morgan Kaufmann, San Mateo, CA, D.S. Touretzky (Ed.); Caruana, R., Multitask learning (1997) Machine Learning, 28 (1), pp. 41-75; Casey, M.P., The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction (1996) Neural Computation, 8 (6), pp. 1135-1178; Cauwenberghs, G., A fast stochastic error-descent algorithm for supervised learning and optimization (1993) Advances in neural information processing systems, vol. 5, p. 244. , Morgan Kaufmann, D.S. Lippman, J.E. Moody, D.S. Touretzky (Eds.); Chaitin, G.J., On the length of programs for computing finite binary sequences (1966) Journal of the ACM, 13, pp. 547-569; Chalup, S.K., Blair, A.D., Incremental training of first order recurrent neural networks to predict a context-sensitive language (2003) Neural Networks, 16 (7), pp. 955-972; Chellapilla, K., Puri, S., Simard, P., High performance convolutional neural networks for document processing. (2006) International workshop on Frontiers in handwriting recognition.; Chen, K., Salman, A., Learning speaker-specific characteristics with a deep neural architecture (2011) IEEE Transactions on Neural Networks, 22 (11), pp. 1744-1756; Cho, K., (2014) Foundations and advances in deep learning, , (Ph.D. thesis), Aalto University School of Science; Cho, K., Ilin, A., Raiko, T., Tikhonov-type regularization for restricted Boltzmann machines (2012) Intl. conf. on artificial neural networks 2012, pp. 81-88. , Springer; Cho, K., Raiko, T., Ilin, A., Enhanced gradient for training restricted Boltzmann machines (2013) Neural Computation, 25 (3), pp. 805-831; Church, A., An unsolvable problem of elementary number theory (1936) The American Journal of Mathematics, 58, pp. 345-363; Ciresan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J., Deep neural networks segment neuronal membranes in electron microscopy images (2012) Advances in neural information processing systems (NIPS), pp. 2852-2860; Ciresan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J., Mitosis detection in breast cancer histology images with deep neural networks (2013) Proc. MICCAI, 2, pp. 411-418; Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J., Deep big simple neural nets for handwritten digit recogntion (2010) Neural Computation, 22 (12), pp. 3207-3220; Ciresan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J., Flexible, high performance convolutional neural networks for image classification (2011) Intl. joint conference on artificial intelligence, pp. 1237-1242; Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J., A committee of neural networks for traffic sign classification (2011) International joint conference on neural networks, pp. 1918-1921; Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J., Multi-column deep neural network for traffic sign classification (2012) Neural Networks, 32, pp. 333-338; Ciresan, D.C., Meier, U., Schmidhuber, J., Multi-column deep neural networks for image classification (2012) IEEE Conference on computer vision and pattern recognition, , arxiv:1202.2745v1, Long preprint [cs.CV]; Ciresan, D.C., Meier, U., Schmidhuber, J., Transfer learning for Latin and Chinese characters with deep neural networks (2012) International joint conference on neural networks, pp. 1301-1306; Ciresan, D.C., Schmidhuber, J., (2013) Multi-column deep neural networks for offline handwritten Chinese character classification. Technical report, , arxiv:1309.0261, IDSIA; Cliff, D.T., Husbands, P., Harvey, I., Evolving recurrent dynamical networks for robot control (1993) Artificial neural nets and genetic algorithms, pp. 428-435. , Springer; Clune, J., Mouret, J.-B., Lipson, H., The evolutionary origins of modularity (2013) Proceedings of the Royal Society B: Biological Sciences, 280 (1755), p. 20122863; Clune, J., Stanley, K.O., Pennock, R.T., Ofria, C., On the performance of indirect encoding across the continuum of regularity (2011) IEEE Transactions on Evolutionary Computation, 15 (3), pp. 346-367; Coates, A., Huval, B., Wang, T., Wu, D.J., Ng, A.Y., Catanzaro, B., Deep learning with COTS HPC systems (2013) Proc. international conference on machine learning.; Cochocki, A., Unbehauen, R., (1993) Neural networks for optimization and signal processing, , John Wiley & Sons, Inc; Collobert, R., Weston, J., A unified architecture for natural language processing: deep neural networks with multitask learning (2008) Proceedings of the 25th international conference on machine learning, pp. 160-167. , ACM; Comon, P., Independent component analysis-a new concept? (1994) Signal Processing, 36 (3), pp. 287-314; Connor, C.E., Brincat, S.L., Pasupathy, A., Transformation of shape information in the ventral pathway (2007) Current Opinion in Neurobiology, 17 (2), pp. 140-147; Connor, J., Martin, D.R., Atlas, L.E., Recurrent neural networks and robust time series prediction (1994) IEEE Transactions on Neural Networks, 5 (2), pp. 240-254; Cook, S.A., The complexity of theorem-proving procedures (1971) Proceedings of the 3rd annual ACM symposium on the theory of computing, pp. 151-158. , ACM, New York; Cramer, N.L., A representation for the adaptive generation of simple sequential programs (1985) Proceedings of an international conference on genetic algorithms and their applications, Carnegie-Mellon University, , Lawrence Erlbaum Associates, Hillsdale, NJ, J. Grefenstette (Ed.); Craven, P., Wahba, G., Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the method of generalized cross-validation (1979) Numerische Mathematik, 31, pp. 377-403; Cuccu, G., Luciw, M., Schmidhuber, J., Gomez, F., Intrinsically motivated evolutionary search for vision-based reinforcement learning (2011) Proceedings of the 2011 IEEE conference on development and learning and epigenetic robotics IEEE-ICDL-EPIROB, 2, pp. 1-7. , IEEE; Dahl, G.E., Sainath, T.N., Hinton, G.E., Improving deep neural networks for LVCSR using rectified linear units and dropout (2013) IEEE International conference on acoustics, speech and signal processing, pp. 8609-8613. , IEEE; Dahl, G., Yu, D., Deng, L., Acero, A., Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) IEEE Transactions on Audio, Speech and Language Processing, 20 (1), pp. 30-42; D'Ambrosio, D.B., Stanley, K.O., A novel generative encoding for exploiting neural network sensor and output geometry (2007) Proceedings of the conference on genetic and evolutionary computation, pp. 974-981; Datar, M., Immorlica, N., Indyk, P., Mirrokni, V.S., Locality-sensitive hashing scheme based on p-stable distributions (2004) Proceedings of the 20th annual symposium on computational geometry, pp. 253-262. , ACM; Dayan, P., Hinton, G., Feudal reinforcement learning (1993) Advances in neural information processing systems (NIPS), vol. 5, pp. 271-278. , Morgan Kaufmann, D.S. Lippman, J.E. Moody, D.S. Touretzky (Eds.); Dayan, P., Hinton, G.E., Varieties of Helmholtz machine (1996) Neural Networks, 9 (8), pp. 1385-1403; Dayan, P., Hinton, G.E., Neal, R.M., Zemel, R.S., The Helmholtz machine (1995) Neural Computation, 7, pp. 889-904; Dayan, P., Zemel, R., Competition and multiple cause models (1995) Neural Computation, 7, pp. 565-579; Deco, G., Parra, L., Non-linear feature extraction by redundancy reduction in an unsupervised stochastic neural network (1997) Neural Networks, 10 (4), pp. 683-691; Deco, G., Rolls, E.T., Neurodynamics of biased competition and cooperation for attention: a model with spiking neurons (2005) Journal of Neurophysiology, 94 (1), pp. 295-313; De Freitas, J.F.G., (2003) Bayesian methods for neural networks, , (Ph.D. thesis), University of Cambridge; DeJong, G., Mooney, R., Explanation-based learning: an alternative view (1986) Machine Learning, 1 (2), pp. 145-176; DeMers, D., Cottrell, G., Non-linear dimensionality reduction (1993) Advances in neural information processing systems (NIPS), vol. 5, pp. 580-587. , Morgan Kaufmann, S.J. Hanson, J.D. Cowan, C.L. Giles (Eds.); Dempster, A.P., Laird, N.M., Rubin, D.B., Maximum likelihood from incomplete data via the EM algorithm (1977) Journal of the Royal Statistical Society B, 39; Deng, L., Yu, D., (2014) Deep learning: methods and applications, , NOW Publishers; Desimone, R., Albright, T.D., Gross, C.G., Bruce, C., Stimulus-selective properties of inferior temporal neurons in the macaque (1984) The Journal of Neuroscience, 4 (8), pp. 2051-2062; de Souto, M.C., Souto, M.C.P.D., Oliveira, W.R.D., The loading problem for pyramidal neural networks (1999) Electronic Journal on Mathematics of Computation; De Valois, R.L., Albrecht, D.G., Thorell, L.G., Spatial frequency selectivity of cells in macaque visual cortex (1982) Vision Research, 22 (5), pp. 545-559; Deville, Y., Lau, K.K., Logic program synthesis (1994) Journal of Logic Programming, 19 (20), pp. 321-350; de Vries, B., Principe, J.C., A theory for neural networks with time delays (1991) Advances in neural information processing systems (NIPS), 3, pp. 162-168. , Morgan Kaufmann, R.P. Lippmann, J.E. Moody, D.S. Touretzky (Eds.); DiCarlo, J.J., Zoccolan, D., Rust, N.C., How does the brain solve visual object recognition? (2012) Neuron, 73 (3), pp. 415-434; Dickmanns, E.D., Behringer, R., Dickmanns, D., Hildebrandt, T., Maurer, M., Thomanek, F., The seeing passenger car 'VaMoRs-P' (1994) Proc. int. symp. on intelligent vehicles, pp. 68-73; Dickmanns, D., Schmidhuber, J., Winklhofer, A., (1987) Der genetische algorithmus: eine implementierung in prolog. Technical report, , http://www.idsia.ch/~juergen/geneticprogramming.html, Inst. of Informatics, Tech. Univ. Munich; Dietterich, T.G., Ensemble methods in machine learning (2000) Multiple classifier systems, pp. 1-15. , Springer; Dietterich, T.G., Hierarchical reinforcement learning with the MAXQ value function decomposition (2000) Journal of Artificial Intelligence Research (JAIR), 13, pp. 227-303; Di Lena, P., Nagata, K., Baldi, P., Deep architectures for protein contact map prediction (2012) Bioinformatics, 28, pp. 2449-2457; Director, S.W., Rohrer, R.A., Automated network design-the frequency-domain case (1969) IEEE Transactions on Circuit Theory, CT-16, pp. 330-337; Dittenbach, M., Merkl, D., Rauber, A., The growing hierarchical self-organizing map (2000) IEEE-INNS-ENNS International joint conference on neural networks, vol. 6, p. 6015. , IEEE Computer Society; Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., DeCAF: a deep convolutional activation feature for generic visual recognition (2013), ArXiv Preprint. arxiv:1310.1531; Dorffner, G., Neural networks for time series processing (1996) Neural network world.; Doya, K., Samejima, K., Ichi Katagiri, K., Kawato, M., Multiple model-based reinforcement learning (2002) Neural Computation, 14 (6), pp. 1347-1369; Dreyfus, S.E., The numerical solution of variational problems (1962) Journal of Mathematical Analysis and Applications, 5 (1), pp. 30-45; Dreyfus, S.E., The computational solution of optimal control problems with time lag (1973) IEEE Transactions on Automatic Control, 18 (4), pp. 383-385; Duchi, J., Hazan, E., Singer, Y., Adaptive subgradient methods for online learning and stochastic optimization (2011) The Journal of Machine Learning, 12, pp. 2121-2159; Egorova, A., Gloye, A., Göktekin, C., Liers, A., Luft, M., Rojas, R., (2004) FU-fighters small size 2004, team description, , RoboCup 2004 symposium: papers and team description papers. CD edition; Elfwing, S., Otsuka, M., Uchibe, E., Doya, K., Free-energy based reinforcement learning for vision-based navigation with high-dimensional sensory inputs (2010) Neural information processing. theory and algorithms (ICONIP), vol. 1, pp. 215-222. , Springer; Eliasmith, C., (2013) How to build a brain: a neural architecture for biological cognition, , Oxford University Press, New York, NY; Eliasmith, C., Stewart, T.C., Choo, X., Bekolay, T., DeWolf, T., Tang, Y., A large-scale model of the functioning brain (2012) Science, 338 (6111), pp. 1202-1205; Elman, J.L., Finding structure in time (1990) Cognitive Science, 14 (2), pp. 179-211; Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio, S., Why does unsupervised pre-training help deep learning? (2010) Journal of Machine Learning Research, 11, pp. 625-660; Escalante-B, A.N., Wiskott, L., How to solve classification and regression problems on high-dimensional data with a supervised extension of slow feature analysis (2013) Journal of Machine Learning Research, 14, pp. 3683-3719; Eubank, R.L., Spline smoothing and nonparametric regression (1988) Self-organizing methods in modeling, , Marcel Dekker, New York, S. Farlow (Ed.); Euler, L., (1744), Methodus inveniendi; Eyben, F., Weninger, F., Squartini, S., Schuller, B., Real-life voice activity detection with LSTM recurrent neural networks and an application to Hollywood movies (2013) Proc. 38th IEEE international conference on acoustics, speech, and signal processing, pp. 483-487; Faggin, F., Neural network hardware (1992) International joint conference on neural networks, 1, p. 153; Fahlman, S.E., (1988) An empirical study of learning speed in back-propagation networks. Technical report CMU-CS-88-162, , Carnegie-Mellon Univ; Fahlman, S.E., The recurrent cascade-correlation learning algorithm (1991) Advances in neural information processing systems (NIPS), 3, pp. 190-196. , Morgan Kaufmann, R.P. Lippmann, J.E. Moody, D.S. Touretzky (Eds.); Falconbridge, M.S., Stamps, R.L., Badcock, D.R., A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images (2006) Neural Computation, 18 (2), pp. 415-429; Fan, Y., Qian, Y., Xie, F., Soong, F.K., TTS synthesis with bidirectional LSTM based recurrent neural networks (2014) Proc. Interspeech.; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2013) IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (8), pp. 1915-1929; Farlow, S.J., (1984) Self-organizing methods in modeling: GMDH type algorithms, vol. 54, , CRC Press; Feldkamp, L.A., Prokhorov, D.V., Eagen, C.F., Yuan, F., Enhanced multi-stream Kalman filter training for recurrent networks (1998) Nonlinear modeling, pp. 29-53. , Springer; Feldkamp, L.A., Prokhorov, D.V., Feldkamp, T.M., Simple and conditioned adaptive behavior from Kalman filter trained recurrent networks (2003) Neural Networks, 16 (5), pp. 683-689; Feldkamp, L.A., Puskorius, G.V., A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification (1998) Proceedings of the IEEE, 86 (11), pp. "